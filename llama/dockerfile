FROM nvidia/cuda:12.2.0-base-ubuntu22.04

# Set environment variables for non-interactive installation
ENV DEBIAN_FRONTEND=noninteractive

# Set OLLAMA_HOST environment variable
ENV OLLAMA_HOST=0.0.0.0:11434

# Update apt-get and install required dependencies
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    curl \
    wget \
    git \
    && rm -rf /var/lib/apt/lists/*

# Install Ollama and any other necessary tools
RUN curl -fsSL https://ollama.com/install.sh | sh

# Ensure ollama is available
RUN echo "Checking Ollama version" && ollama --version

# Run Ollama in the background and pull the model
RUN ollama serve & \
    sleep 5 && \
    ollama pull llama2

# Start Ollama when the container runs
CMD ["ollama", "serve"]
